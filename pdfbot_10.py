# pdfbot_10.py
# LangGraph + Ollama Q&A (fallback iÃ§eren sÃ¼rÃ¼m)

import json
from typing import Any, Dict, List

# LLM importu: gÃ¼ncel paketleri deneniyor (hangisi kuruluysa onu kullan)
try:
    from langchain_ollama import ChatOllama  # type: ignore
except Exception:
    try:
        from langchain_community.chat_models import ChatOllama  # type: ignore
    except Exception:
        try:
            from langchain.chat_models import ChatOllama  # type: ignore
        except Exception:
            ChatOllama = None  # type: ignore

# LangGraph importu: yoksa fallback yapacaÄŸÄ±z
try:
    from langgraph.graph import StateGraph  # type: ignore
    # MemorySaver genelde tip bilgisi olmuyor; try/except ile al
    try:
        from langgraph.checkpoint import MemorySaver  # type: ignore
    except Exception:
        MemorySaver = None  # type: ignore
except Exception:
    StateGraph = None  # type: ignore
    MemorySaver = None  # type: ignore

# Hangi modeli kullanacaÄŸÄ±mÄ±zÄ± belirle 
LLM_MODEL_NAME = "gemma3:4b"

# LLM baÅŸlat (varsa)
if ChatOllama is None:
    raise RuntimeError("ChatOllama bulunamadÄ±. lÃ¼tfen 'langchain-ollama' veya 'langchain_community' paketlerini kurun.")
llm = ChatOllama(model=LLM_MODEL_NAME)  # type: ignore

# Veri kaynaÄŸÄ± (gÃ¼venli JSON)
DATA_FILE = "emsal_safe.json"
with open(DATA_FILE, "r", encoding="utf-8") as f:
    data: List[Dict[str, Any]] = json.load(f)

# ---------- Basit retrieval fonksiyonu ----------
def retrieve_context_simple(question: str, max_chars: int = 4000) -> str:
    """Basit kelime eÅŸleÅŸmesiyle ilgili Ã¶zet parÃ§alarÄ±nÄ± birleÅŸtirir."""
    q = question.lower().strip()
    if not q:
        # hepsini dÃ¶ndÃ¼r
        parts = [item.get("summary", "") for item in data]
        return "\n\n".join(parts)[:max_chars]

    # basit token eÅŸleÅŸme: soru kelimelerinden ilk 6'sÄ±nÄ± kullan
    tokens = [t for t in q.split() if t]
    tokens = tokens[:6]

    hits: List[str] = []
    for item in data:
        txt = (item.get("summary") or "").lower()
        # EÄŸer herhangi bir token summary iÃ§inde geÃ§iyorsa al
        if any(tok in txt for tok in tokens):
            hits.append(item.get("summary", ""))

    if not hits:
        hits = [item.get("summary", "") for item in data]

    ctx = "\n\n".join(hits)
    if len(ctx) > max_chars:
        return ctx[:max_chars] + "..."
    return ctx

# ---------- LangGraph dÃ¼ÄŸÃ¼m fonksiyonlarÄ± ----------
# Tipleri basit dict kullanan yapÄ± ile yazÄ±yoruz (Pylance daha az ÅŸikayet)
def retrieve_context_node(state: Dict[str, Any]) -> Dict[str, Any]:
    question = state.get("question", "") or ""
    state["context"] = retrieve_context_simple(question)
    return state

def generate_answer_node(state: Dict[str, Any]) -> Dict[str, Any]:
    question = state.get("question", "") or ""
    context = state.get("context", "") or ""
    prompt = f"Soru: {question}\n\nBaÄŸlam:\n{context}\n\nKÄ±sa, net ve TÃ¼rkÃ§e cevap ver:"
    # llm.invoke / llm() farklÄ± versiyonlarda deÄŸiÅŸebilir; denemelerle uyumlu hale getiriyoruz
    try:
        # Ã¶ncelikle invoke'Ä± dene 
        resp = llm.invoke(prompt)  # type: ignore
        answer = getattr(resp, "content", resp)
        if isinstance(answer, dict):
            # bazÄ± implementasyonlarda dict gelebiliyor
            answer = answer.get("content") or str(answer)
    except Exception:
        try:
            # fallback: Ã§aÄŸÄ±rma gibi davran
            resp = llm(prompt)  # type: ignore
            if isinstance(resp, str):
                answer = resp
            elif hasattr(resp, "content"):
                answer = getattr(resp, "content")
            else:
                answer = str(resp)
        except Exception as e:
            answer = f"LLM Ã§aÄŸrÄ±sÄ±nda hata: {e}"

    state["answer"] = str(answer)
    return state

# ---------- Graph oluÅŸturma (varsa LangGraph ile) ----------
if StateGraph is not None:
    # StateGraph tipleri Pylance ile Ã§akÄ±ÅŸtÄ±ÄŸÄ± iÃ§in # type: ignore kullanÄ±yoruz burada
    graph = StateGraph(dict)  # type: ignore
    graph.add_node("retriever", retrieve_context_node)  # type: ignore
    graph.add_node("llm", generate_answer_node)  # type: ignore
    graph.set_entry_point("retriever")  # type: ignore
    if MemorySaver is not None:
        try:
            memory = MemorySaver()  # type: ignore
            app = graph.compile(checkpointer=memory)  # type: ignore
        except Exception:
            app = graph.compile()  # type: ignore
    else:
        app = graph.compile()  # type: ignore

    def run_query(question: str) -> str:
        state = {"question": question}
        result = app.invoke(state)  # type: ignore
        return str(result.get("answer", "") or result.get("context", "").splitlines()[0] or "")
else:
    # LangGraph yÃ¼klÃ¼ deÄŸilse basit pipeline
    def run_query(question: str) -> str:
        ctx = retrieve_context_simple(question)
        state = {"question": question, "context": ctx}
        state = generate_answer_node(state)
        return str(state.get("answer", ""))

# ---------- Basit CLI ----------
if __name__ == "__main__":
    print("ğŸ“– PDF Chatbot hazÄ±r! Ã‡Ä±kmak iÃ§in 'q' yaz.\n")
    while True:
        q = input("Sorunuzu yazÄ±n: ")
        if q.strip().lower() == "q":
            break
        ans = run_query(q)
        print("\nğŸ¤– Cevap:", ans, "\n")
